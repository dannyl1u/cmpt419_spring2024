---
title: "Training influence, part 2"
date: "2024-02-07"
author: "Prof. Nick Vincent"
institute: "Simon Fraser University"
jupyter: python3
---

## Agenda

- Review training gradients

## Quick review of training gradients

Imagine we're trying to predict a single output value $y$ from a single input value $x$ using a simple neural network. The network's prediction $\hat{y}$ is given by $\hat{y} = wx + b$

Where:

- $w$ is the weight of the connection between the input and output neuron.
- $b$ is the bias.
- $x$ is the input.

## 

```{mermaid}
graph TD
    X(Input x) -->|Weight w| Y(Temporary Output)
    B(Bias b) --> Y
    Y -->|Sum| Z(Output y)
```

## Objective of our training

- want to adjust $w$ and $b$ to get $\hat{y}$ close to real $y$
- measure how we're doing with *loss*
- example choice: MSE, $L = \frac{1}{2}(y - \hat{y})^2$

## Training Gradients in this example

Training gradients indicate in which direction (and by how much) we should adjust our params ($w$ and $b$) to minimize loss

To find these gradients, we use backpropagation, which involves calculating the derivative of the loss function with respect to each parameter

## Note on terms

In our reading, we uses $\theta$ to refer to a vector of arbitrary number of params (cardinality is $p$).

So in this example we can assume $\theta = <w, b>$

## Gradient wrt $w$

$\frac{\partial L}{\partial w} = -(y - \hat{y} ) \cdot x$

tells how a small change in $w$ affects loss. 

If $\frac{\partial L}{\partial w}$ is positive, increasing $w$ will increase the loss, so we should decrease $w$ to reduce the loss.

## How did we get gradient wrt $w$?

- plugin $\hat{y}$ into $L$
- $L = \frac{1}{2}(y - (wx+b))^2$
- apply chain rule
- $\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial w}$

## How did we get gradient wrt w?
- $\frac{\partial L}{\partial \hat{y}} = \frac{\partial}{\partial \hat{y}} * (\frac{1}{2}(y - \hat{y})^2) = -(y - \hat{y}) = \hat{y} - y$
- $\frac{\partial \hat{y}}{\partial w} = \frac{\partial}{\partial w} (wx+b) = x$

## Chain rule is optional

- Note that we can also just multiply everything out and compute partial derivatives without the chain rule
- Chain rule is just convenient for composite function. Here $\hat{y}$ is a function of w, x, and b.

## Example with actual values, 1/n

Suppose

- Input value $x=2$
- Actual output value $y=5$
- Weight $w=1$
- Bias $b=1$

Want gradient of $L$ wrt $w$ still

## Example with actual values, 2/n

- $\hat{y} = wx+b = 1*2+1 = 3$
- $L = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(5 - 3)^2=2$
- $\frac{\partial L}{\partial w} = -(y - \hat{y})x = -(5-3)*2 = -4$

## Example with actual values, 3/n

- Small increase in $w$ will decrease loss, so adjust $w$ upwards
- We make our update based on learning rate


## Gradient wrt $b$

$\frac{\partial L}{\partial b} = -(y - \hat{y})$

tells us how a small change in $b$ affects the loss. Similarly, if $\frac{\partial L}{\partial b}$ is positive, increasing $b$ will increase the loss, so we should decrease $b$ to reduce the loss.

## How did we get gradient wrt $b$

- $\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial b}$ 
- $\frac{\partial L}{\partial \hat{y}} = -(y - \hat{y})$
- $\frac{\partial \hat{y}}{\partial b} = \partial \hat{y} (wx+b) = 1$
- $\frac{\partial L}{\partial b} = -(y - \hat{y})$

##

We update $w$ and $b$ using a learning rate $\eta$ (a kind of step size, how far we adjust things based on the direction of our gradients):

$w = w - \eta \frac{\partial L}{\partial w}$

$b = b - \eta \frac{\partial L}{\partial b}$


## Iteration

This process is repeated for many iterations (or epochs) over the training data, gradually reducing the loss and making the predictions $\hat{y}$ closer to the actual outputs $y$


## Other resources

- We ignored activation function here
- Exercise for the reader! What if we add ReLu or logistic activation function (hint: now we have $z=wx+b$ and $\hat{y}=a(z)$, so we have to do a 3-piece chain rule)
- See e.g. https://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/slides/lec04.pdf

## Relevance back to training data influence

- Gradient based methods rely on the idea that: training data only influences the final model via the gradients produced
- We should be able to keep track of these gradients to understand how a training instance affected a test instance


## For a data valuator

- We can assume gradients are being calculated throughout training
- pytorch: `requires_grad=True`
- another example: https://fluxml.ai/Flux.jl/stable/models/basics/


## Methods covered thus far


## Leave-one-out influence (retraining-based)

- remove an instance and see the change in risk
- time complexity $O(T)$ for one retrain
- so $O(nT)$ for n data points
- space complexity $O(n+p)$ (hold $n$ values, each retrain has to hold $p$ params). (exact value depends on concurrency)
- trade-off in terms of discarding models 

## LOO pros and cons

- simple, easy to understand
- used for fairness, see [BF21] in H+L
- but: extremely expensive!
- need to retrain even more if you want to account for variance

## Notes

- can efficiently calculate for KNN, see [Jia+21a] in H+L
- $O(n log(n))$ to calculate very close to exact value
- can also efficiently calculate for decision tree, see [Sha+18b]
- can efficiently estimate for linear regression
- can easily extend to "leave $m$ out", $O(n \choose m)$

##

- So, while it's very expensive to get LOO values, there's a bunch of scenarios in which we can get actually get them

## Downsampling (retraining-based)

- Train an ensemble of submodels
- calculate change in average risk when $z_i$ is not used
- consistent estimator of LOO

## Downsampling

- instead of depending on size $n$, our complexity depends on our selected number of submodels $K$. More submodels costs more but improves quality of the estimator
- still expensive!

## Shapley values (retraining-based)

- from cooperative game theory
- idea: give people an idea about their "value added" in a game where they might choose from a variety of teams
- if we treat each instance as a "player" and imagine all the possible "teams" that might exist, we start to get a Shapley value

## Shapley values

- it's the weighted impact on risk when $z_i$ is added to a random training subset of any size
- we don't have to weight all sizes equally, though
- LOO influence that accounts for all possible subsets of D

## Notes on Shapley

- also used for feature explanations (though some issues, see https://proceedings.mlr.press/v119/kumar20e.html)

## Shapley properties

- researchers like the nice theoretical properties
- dummy player = player $i$ adds same value to any coalition as amount they receive on their own
- symmetrical
- linear
- accounts for multiple training set sizes

## Shapley caveats

- super expensive. Worst case exponential time (pending P=NP...)
- $2^n$ models, not $n$ models...
- can be estimated


## Influence functions (gradient based)

- "training instances only influence a model through training gradients"
- use Taylor series approximations and some assumptions
- static vs dynamic: do we just focus on the fixed final params
- major limitation wrt high influence

## Assumptions

- stationarity: model params have converged
- convexity^[https://en.wikipedia.org/wiki/Jensen%27s_inequality]

## Basic idea

- if model *f* and loss *L* are twice-differentiable and strictly convex, we can calculate the change in loss when a training instance is upweighted by $\epsilon$ 
- using closed form expression that takes into account the Hessian (based on second derivative of loss wrt params) and gradient (first derivative of loss wrt params)

##

- TLDR: if we have the gradients and Hessians^[https://en.wikipedia.org/wiki/Hessian_matrix] we can compute in closed form how model loss will change with $z_i$ is removed

## What should we know for now

- If we want to do active maths research in this area, good to understand everything in the paper
- For our purposes, we want to understand the big idea behind each kind of data value
- Aim to understand the intuitive description of what's going on (H+L make an effort to provide this)
- Understand why the methods vary in time and space complexity

## What does this all have to do with human-centered AI

All these methods estimate what will happen in some counterfactual world in which the data changes.

What if people actually cause that data to change?

