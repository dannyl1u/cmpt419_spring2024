---
title: "Intro"
date: "2024-01-15"
author: "Prof. Nick Vincent"
institute: "Simon Fraser University"
jupyter: python3
---
# About

This deck contains lecture content for Jan 15, 2024 and Jan 24, 2024.

## {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #00ff9f; width: 200px; height: 150px; margin: 10px;"}
:::

::: {data-id="box2" auto-animate-delay="0.1" style="background: #00b8ff; width: 200px; height: 150px; margin: 10px;"}
:::

::: {data-id="box3" auto-animate-delay="0.2" style="background: #001eff; width: 200px; height: 150px; margin: 10px;"}
:::

::: {data-id="box4" auto-animate-delay="0.2" style="background: #bd00ff; width: 200px; height: 150px; margin: 10px;"}
:::

:::

## {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-stack}
::: {data-id="box1" style="background: #00ff9f; width: 350px; height: 350px; border-radius: 200px;"}
:::

::: {data-id="box2" style="background: #00b8ff; width: 250px; height: 250px; border-radius: 200px;"}
:::

::: {data-id="box3" style="background: #001eff; width: 150px; height: 150px; border-radius: 200px;"}
:::

::: {data-id="box4" style="background: #bd00ff; width: 50px; height: 50px; border-radius: 200px;"}
:::
:::


# Block 1

For the Spring 2024 term, this was presented on 2024-01-15.

# While we file in

- Check out Piazza link (Canvas announcements)
- Anyone have AI-related news they want to discuss
- Feel free to collect your thoughts re: strong reactions to readings

# Course Logistics

- Tentative course calendar on Canvas updated
- Slides will go on GitHub
- Submit reading response by Wed. this week (or email me if you just joined)
- New readings posted

# Agenda

- Today: Discuss readings for this week
- Goals: highlight key takeaways, find areas of interest
- At the end: Intro to readings for next week

# HCML Reading (Chancellor 2023)

## Good and bad uses of AI -- some go to examples

::: {.incremental}
- Auto complete
- Predict malignant tumor
- Deep fake
- Discrimination (on many axes)
:::

## Progress? in AI

:::: {.columns}
::: {.column width="25%"}
- Deep learning "gains" don't always hold up to scrutinty
:::

::: {.column width="70%"}
![](figs/worrying_trends.png)
:::
::::


::: aside
There are more examples we can discuss! And of course we can keep an eye out for new research news.
:::


## People as "objects of prediction"

- How to counter this?
  - Fairness, equality, justice
- Note: computing often forces us to precisely define these concepts in a way that's illuminating
- We might disagree


## Sub-fields of CS

Article mentions we should "refine HCML into a unifying and interdisciplinary force across CS rather than risk fracture with each sub-field of CS taking ownership of an independent vision of HCML"

What's the backstory here?

## A bit of behind the scenes into CS research

- Subcommunities often run their own conferences
- Drives a lot of the incentives of researchers
- This might matter for research-related jobs in industry too
- e.g., some ML jobs list NeurIps papers as a requirement, some Responsible AI jobs list FAccT

## Examples

- ML people send papers to NeurIPS, ICML, ICLR, and many more
- HCI people send papers to CHI, CSCW, and many more
- Philosophy people send papers to their own, pretty much entirely separate journals

## Early HCML

- Chancellor highlights some of the history -- the "HCI" community and "FAccT" community played major roles
  - FAccT is a relatively new conference which has gained a lot of momentum
- Information Science and STS
- CSCW also plays a major role
- Various social sciences and fields of critical scholarships as well..
- That's a lot of disciplines already

## Acronym Cheatsheet thus far

- HCI: human-computer interaction. Main conference is "CHI", confusingly.
- FAccT: Fairness, Accountability, and Transparency
- CSCW: computer supported cooperative work (and social computing)
- [STS](https://en.wikipedia.org/wiki/Science_and_technology_studies): Science and technology studies. Also, science, technology, and society
 
# What Counts as HCML?

## Proposed definition statement

"Human-centered machine learning is a set of practices for building, evaluating, deploying, and critiquing ML systems that balances technical innovation with an equivalent focus on human and social concerns."^[[Chancellor 2023](https://cacm.acm.org/magazines/2023/3/270209-toward-practices-for-human-centered-machine-learning/fulltext)]

## We can evaluate different practices to see if they fit in this set

- It's possible two different organizations both trying to build an AI system for the same *task* could differ substantially in whether they meet the "human-centered" definition
- People can always disagree about what human and social concerns should be ranked most highly
- But this gives us a starting point

## Focusing on Practices 

Five categories of suggestions are given, i.e. what can you do when you're a software engineer, manager, research scientist, professor, etc.

- should I use ML?
- what's my "position"? ^[See e.g. [positionality statements](https://en.wikipedia.org/wiki/Positionality_statement)]
- users vs. humans
- credit other domains
- iterate on failure

# What challenges might we expect to face?

## Institutional actions

- new norms at conference, e.g. negative impact statements (NeurIPS)
- institutional support for interdisciplinary research
- computing (broad) vs. computer science
- support students who want to do interdisciplinary research!

# Over to DCAI

## Problems with data

- "Differences in labeling": do you and I agree if a pill is "scratched"? Does my hospital notes system have a different coding system than yours?
- "Emphasis on big data": what about a rare medical condition?
- "Ad hoc data curation": need to systemize?

## Finding label disputes

- We might use tools to find subsets of a dataset with high label dispute
- Influence estimation provides one approach we'll see

## Domain Expertise

- get the biologists to label the cells!
- get former players to provide "labels" for sports analytics
- many more examples
- this is where the DCAI argument really starts to merge with the HCAI argument

## More info

- https://www.youtube.com/watch?v=TU6u_T-s68Y


## What is DataPerf

- a so-called "benchmark suite"
- focused on data tasks
- meant to be community run and led 


## What's a "ML benchmark?"

Conventional model-centric ML definition: "a standard, fixed dataset for model accuracy comparisons and per-
formance measurement" (p2, Mazumber et al)

## Some terms

- from "Probabilistic Machine Learning: An Introduction", Murphy 2022 (https://probml.github.io/pml-book/book1.html)
- task $T$ to learn mapping $f$ from inputs $x \in X$ to outputs $y \in Y$
- x called features (or covariates, or predictors)
- y is label (or target, or response)
- we have N input-output pairs $D = {(x_n, y_n)}$ for $n \in (1,N)$. D is the training set, N is the sample size.

## Comparing model-centric benchmark and data-centric benchmark

- in model-centric, we have a fixed dataset $D$ and we try a bunch of different ways to find $f$
- change model architecture, change training hyperparameters, change task metrics
- in data-centric, we keep all these fixed and just change $D$

## Testable concept: is a benchmark data centric

- you might imagine a test question that describes several differents tasks and asks you to identify which one is "data-centric"


## Six data centric operations

data...

- parsing
- augmentation
- selection
- quality assessment
- acquisition
- cleaning

##

Hold time for:

- discuss next week's readings
- questions that have popped up thus far
