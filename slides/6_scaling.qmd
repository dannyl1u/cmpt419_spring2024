---
title: "Data scaling"
date: "2024-02-28"
author: "Prof. Nick Vincent"
institute: "Simon Fraser University"
jupyter: python3
---

## Slide for logistics and news


## Agenda for this deck

- Discuss key takeaways from the Hestness data scaling paper
- "Where are they now?" - How do these results hold up in 2024?
  - Discuss the data scaling database


## Hestness data scaling paper

- Note that this paper discusses both data scaling and compute scaling (we haven't talked about compute much)

In short: this paper reports on a number of experiments that involve training models with different sized datasets with the ultimate goal of predicting "if has x times more data, how much better would our model be?"

## Key Takeaway #1: Power Laws in Theory

Theory says: $Œµ(m) ‚àù Œ±m ^ {Œ≤_g}$

- Œµ is generalization error
- m is "number of samples"
- Œ± (alpha) is constant (specific to task)
- $Œ≤_g$ is scaling exponent (specific to task). $Œ≤_g$ should be -0.5 or -1 in theory, but in practice is more like -0.07 to -0.35


## Power Laws in Practice

When you run the big expensive experiments... it does seem like power law describe the scaling pretty well!

- holds across models, optimizers, regularizers, and loss function choices
- steepness is domain specific (picking a better model or optimizer just improves the intercept)


## Regions of scaling

- small training set region (i.e. random guess)
- Power law scaling region
- Irreducibile error region aka Bayes region aka flat zone

## Quick run-down on history

- lots of old efforts to explain data scaling in theory (e.g. provide theoretical bounds on how much data do I need to get a certain performance)

## Aside: the coin flip model in the appendix

The appendix has a cool theorem in the appendix ("POWER-LAW LEARNING CURVE FOR COUNTING MODEL CLASSIFIER")

- We have a coin flip "probability estimator"
- We count how many times we get heads 

## What about model parameters

We've mostly ingored for now. We talking about data-centric tasks, its convenient to just assume away -- "let's assume we've got a big enough model and a decent enough architecture". In practice, these things are intertwined.

A nice quote on the topic from Hestness et al. :)

"Rather than reason through these complexities, it is currently easier for researchers and practitioners to over-parameterize models to fit training data"

## Key Takeaway: Using data scaling to estimating costs of progress

## Hestness et al. experiments

Lots of interesting details on experiments! For 419 -- we won't test on this, but worth looking at if you're interested.

- Subdivide datasets into shards using powers of two (0.1%, 0.2%, 0.4%, 0.8%...)
- single held out test set
- find the best model for each dataset size (expensive!)


## On loss function choice

"We report validation losses that are sums or unweighted averages over distance metrics measuring
the error per model-predicted output"

- loss function for the exact characteristics of the learning curve

## Very quick summary of results

- Machine translation: -0.128
- Language modeling: -0.09 to -0.06
- Image classification: -0.309 to -0.488 (depends on eval procedure, top 1 vs top 5)
- Speech recognition: -0.299

Do not need to memorize these! Key idea: "probably dependent on aspects of the problem domain or data distribution" (Sec 5.1)

## Are we *sure* it's a power law?



## Just get more data?

Seems like "just get more data" is a pretty good strategy.

Furthermore, we could do some very basic economic reasoning.


## Are we in the flat zone?

Authors provide three reasons for irreducible error:

- mislabeled samples (training data influence might help with that... ü§î)
- information theoretic lower bound (i.e. how "model-able" is our fundamental data generating functions)

None of their experiments got their. One open question for gen AI companies and other users of "web-scale" data is figuring this out!

## Key Takeaway #2: Relationship between data scaling and influence

Training data influence methods all tell us about counterfactual worlds -- what if we had more data, what if we had less data.

So does data scaling.

Training data influence focuses on specific data points -- what if we had more of a certain group. Data scaling focuses on random draws across a distribution /  data-generating function.

## Data scaling data base today

See Villalobos https://epochai.org/blog/scaling-laws-literature-review


## Equations to understand the regions

Revisiting Neural Scaling Laws in Language and Vision - Alabdulmohsin et al. (Google) - https://arxiv.org/abs/2209.06640v2


- $\frac{\varepsilon_x - \varepsilon_\infty}{(\varepsilon_0 - \varepsilon_x)^\alpha} = \beta x^c $


Broken Neural Scaling Laws- Caballero et al. - https://arxiv.org/abs/2210.14891 - https://github.com/ethancaballero/broken_neural_scaling_laws



